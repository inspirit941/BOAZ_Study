{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5504], grad_fn=<AddBackward0>)\n",
      "<generator object Module.parameters at 0x127110318>\n",
      "[Parameter containing:\n",
      "tensor([[ 0.4346,  0.1119, -0.0582, -0.4244,  0.2762]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3027], requires_grad=True)]\n",
      "Parameter containing:\n",
      "tensor([0.3027], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Linear Function\n",
    "# input dim = 5, output dim = 1\n",
    "linear = nn.Linear(5, 1)\n",
    "x = torch.FloatTensor([-2, -1, 0, 1, 2])\n",
    "print(linear(x)) \n",
    "print(linear.parameters())\n",
    "print([x for x in linear.parameters()])\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n",
      "tensor([0., 0., 0., 1., 2.])\n",
      "tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])\n",
      "tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])\n",
      "tensor([0.0117, 0.0317, 0.0861, 0.2341, 0.6364])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([-2, -1, 0, 1, 2])\n",
    "print(x)\n",
    "\n",
    "# ReLU\n",
    "relu = nn.ReLU()\n",
    "print(relu(x))  # max(0, input)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = nn.Sigmoid() # 고정된 함수에 넣는 거. \n",
    "print(sigmoid(x))\n",
    "\n",
    "# Tanh\n",
    "tanh = nn.Tanh()\n",
    "print(tanh(x))\n",
    "\n",
    "# Softmax\n",
    "softmax = nn.Softmax(dim = 0)\n",
    "print(softmax(x))\n",
    "\n",
    "# softmax와 sigmoid는 다르다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3333)\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([1, 4, 2])\n",
    "b = torch.FloatTensor([1, 3, 2])\n",
    "\n",
    "# MSE Loss\n",
    "mse = nn.MSELoss()\n",
    "print(mse(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0548)\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([[1, 4, 2], [0.5, 0.3, 0.2]])\n",
    "b = torch.LongTensor([0, 0]) # label값.\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "print(cross_entropy(a,b)) # 학습할 x값을 a 위치, 정답을 b 위치.="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# MNIST Dataset DataLoader\n",
    "train_dataset = datasets.MNIST(root='./data', train=True,\n",
    "                        transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False,\n",
    "                        transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), #kernal size = filter size. 16은 필터 개수와 동일.\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = CNN()\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1875], Loss: 0.3772\n",
      "Epoch [1/5], Step [200/1875], Loss: 0.2697\n",
      "Epoch [1/5], Step [300/1875], Loss: 0.1680\n",
      "Epoch [1/5], Step [400/1875], Loss: 0.2900\n",
      "Epoch [1/5], Step [500/1875], Loss: 0.1734\n",
      "Epoch [1/5], Step [600/1875], Loss: 0.2310\n",
      "Epoch [1/5], Step [700/1875], Loss: 0.1478\n",
      "Epoch [1/5], Step [800/1875], Loss: 0.0703\n",
      "Epoch [1/5], Step [900/1875], Loss: 0.0317\n",
      "Epoch [1/5], Step [1000/1875], Loss: 0.0087\n",
      "Epoch [1/5], Step [1100/1875], Loss: 0.1166\n",
      "Epoch [1/5], Step [1200/1875], Loss: 0.0799\n",
      "Epoch [1/5], Step [1300/1875], Loss: 0.0158\n",
      "Epoch [1/5], Step [1400/1875], Loss: 0.0952\n",
      "Epoch [1/5], Step [1500/1875], Loss: 0.0652\n",
      "Epoch [1/5], Step [1600/1875], Loss: 0.0604\n",
      "Epoch [1/5], Step [1700/1875], Loss: 0.0390\n",
      "Epoch [1/5], Step [1800/1875], Loss: 0.0298\n",
      "Epoch [2/5], Step [100/1875], Loss: 0.3216\n",
      "Epoch [2/5], Step [200/1875], Loss: 0.1071\n",
      "Epoch [2/5], Step [300/1875], Loss: 0.1038\n",
      "Epoch [2/5], Step [400/1875], Loss: 0.0083\n",
      "Epoch [2/5], Step [500/1875], Loss: 0.0220\n",
      "Epoch [2/5], Step [600/1875], Loss: 0.3130\n",
      "Epoch [2/5], Step [700/1875], Loss: 0.0289\n",
      "Epoch [2/5], Step [800/1875], Loss: 0.0515\n",
      "Epoch [2/5], Step [900/1875], Loss: 0.0143\n",
      "Epoch [2/5], Step [1000/1875], Loss: 0.1019\n",
      "Epoch [2/5], Step [1100/1875], Loss: 0.0730\n",
      "Epoch [2/5], Step [1200/1875], Loss: 0.0042\n",
      "Epoch [2/5], Step [1300/1875], Loss: 0.0252\n",
      "Epoch [2/5], Step [1400/1875], Loss: 0.0542\n",
      "Epoch [2/5], Step [1500/1875], Loss: 0.0300\n",
      "Epoch [2/5], Step [1600/1875], Loss: 0.0705\n",
      "Epoch [2/5], Step [1700/1875], Loss: 0.0642\n",
      "Epoch [2/5], Step [1800/1875], Loss: 0.1166\n",
      "Epoch [3/5], Step [100/1875], Loss: 0.0780\n",
      "Epoch [3/5], Step [200/1875], Loss: 0.0163\n",
      "Epoch [3/5], Step [300/1875], Loss: 0.0092\n",
      "Epoch [3/5], Step [400/1875], Loss: 0.0742\n",
      "Epoch [3/5], Step [500/1875], Loss: 0.1139\n",
      "Epoch [3/5], Step [600/1875], Loss: 0.0159\n",
      "Epoch [3/5], Step [700/1875], Loss: 0.0787\n",
      "Epoch [3/5], Step [800/1875], Loss: 0.0229\n",
      "Epoch [3/5], Step [900/1875], Loss: 0.0186\n",
      "Epoch [3/5], Step [1000/1875], Loss: 0.0425\n",
      "Epoch [3/5], Step [1100/1875], Loss: 0.0420\n",
      "Epoch [3/5], Step [1200/1875], Loss: 0.0643\n",
      "Epoch [3/5], Step [1300/1875], Loss: 0.0201\n",
      "Epoch [3/5], Step [1400/1875], Loss: 0.0128\n",
      "Epoch [3/5], Step [1500/1875], Loss: 0.0181\n",
      "Epoch [3/5], Step [1600/1875], Loss: 0.0052\n",
      "Epoch [3/5], Step [1700/1875], Loss: 0.0278\n",
      "Epoch [3/5], Step [1800/1875], Loss: 0.0153\n",
      "Epoch [4/5], Step [100/1875], Loss: 0.0892\n",
      "Epoch [4/5], Step [200/1875], Loss: 0.0793\n",
      "Epoch [4/5], Step [300/1875], Loss: 0.0178\n",
      "Epoch [4/5], Step [400/1875], Loss: 0.0216\n",
      "Epoch [4/5], Step [500/1875], Loss: 0.1623\n",
      "Epoch [4/5], Step [600/1875], Loss: 0.0834\n",
      "Epoch [4/5], Step [700/1875], Loss: 0.0154\n",
      "Epoch [4/5], Step [800/1875], Loss: 0.0093\n",
      "Epoch [4/5], Step [900/1875], Loss: 0.0344\n",
      "Epoch [4/5], Step [1000/1875], Loss: 0.0620\n",
      "Epoch [4/5], Step [1100/1875], Loss: 0.0567\n",
      "Epoch [4/5], Step [1200/1875], Loss: 0.1055\n",
      "Epoch [4/5], Step [1300/1875], Loss: 0.0094\n",
      "Epoch [4/5], Step [1400/1875], Loss: 0.0497\n",
      "Epoch [4/5], Step [1500/1875], Loss: 0.0377\n",
      "Epoch [4/5], Step [1600/1875], Loss: 0.0031\n",
      "Epoch [4/5], Step [1700/1875], Loss: 0.0165\n",
      "Epoch [4/5], Step [1800/1875], Loss: 0.0094\n",
      "Epoch [5/5], Step [100/1875], Loss: 0.0119\n",
      "Epoch [5/5], Step [200/1875], Loss: 0.0021\n",
      "Epoch [5/5], Step [300/1875], Loss: 0.0158\n",
      "Epoch [5/5], Step [400/1875], Loss: 0.0561\n",
      "Epoch [5/5], Step [500/1875], Loss: 0.0204\n",
      "Epoch [5/5], Step [600/1875], Loss: 0.0021\n",
      "Epoch [5/5], Step [700/1875], Loss: 0.0230\n",
      "Epoch [5/5], Step [800/1875], Loss: 0.0517\n",
      "Epoch [5/5], Step [900/1875], Loss: 0.0314\n",
      "Epoch [5/5], Step [1000/1875], Loss: 0.0257\n",
      "Epoch [5/5], Step [1100/1875], Loss: 0.0012\n",
      "Epoch [5/5], Step [1200/1875], Loss: 0.1428\n",
      "Epoch [5/5], Step [1300/1875], Loss: 0.0098\n",
      "Epoch [5/5], Step [1400/1875], Loss: 0.0443\n",
      "Epoch [5/5], Step [1500/1875], Loss: 0.0291\n",
      "Epoch [5/5], Step [1600/1875], Loss: 0.0056\n",
      "Epoch [5/5], Step [1700/1875], Loss: 0.0139\n",
      "Epoch [5/5], Step [1800/1875], Loss: 0.0233\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (img, label) in enumerate(train_loader, 1):\n",
    "        img, label = Variable(img), Variable(label)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.78 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for img, label in test_loader:\n",
    "    out = model(img)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    total += label.size(0)\n",
    "    correct += (predicted == label).sum().item()\n",
    "    \n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model_nn.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제1. Neural Network에 활성함수 / batchNorm 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toTensor() => dataloader에서 사용함. 이미지 값을 tensor로 변경해 준다. (type 변경.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuralnetwork(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(Neuralnetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(28*28, 100)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.layer2 = nn.Linear(100,200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.layer3 = nn.Linear(200, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size(0),-1) # x.size는 batch size다. \n",
    "        out = F.relu(self.bn1(self.layer1(out)))\n",
    "        out = F.relu(self.bn2(self.layer2(out)))\n",
    "        out = F.softmax(self.layer3(out), dim = 1) # dim = 1 의미는?\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1875], Loss : 2.2822\n",
      "Epoch [1/5], Step [200/1875], Loss : 2.2484\n",
      "Epoch [1/5], Step [300/1875], Loss : 2.1661\n",
      "Epoch [1/5], Step [400/1875], Loss : 2.1070\n",
      "Epoch [1/5], Step [500/1875], Loss : 2.1090\n",
      "Epoch [1/5], Step [600/1875], Loss : 2.1622\n",
      "Epoch [1/5], Step [700/1875], Loss : 1.9907\n",
      "Epoch [1/5], Step [800/1875], Loss : 1.9820\n",
      "Epoch [1/5], Step [900/1875], Loss : 1.9085\n",
      "Epoch [1/5], Step [1000/1875], Loss : 1.8737\n",
      "Epoch [1/5], Step [1100/1875], Loss : 1.9353\n",
      "Epoch [1/5], Step [1200/1875], Loss : 1.9701\n",
      "Epoch [1/5], Step [1300/1875], Loss : 1.8549\n",
      "Epoch [1/5], Step [1400/1875], Loss : 1.9177\n",
      "Epoch [1/5], Step [1500/1875], Loss : 1.7357\n",
      "Epoch [1/5], Step [1600/1875], Loss : 1.7155\n",
      "Epoch [1/5], Step [1700/1875], Loss : 1.7786\n",
      "Epoch [1/5], Step [1800/1875], Loss : 1.8340\n",
      "Epoch [2/5], Step [100/1875], Loss : 1.6318\n",
      "Epoch [2/5], Step [200/1875], Loss : 1.7387\n",
      "Epoch [2/5], Step [300/1875], Loss : 1.7689\n",
      "Epoch [2/5], Step [400/1875], Loss : 1.7520\n",
      "Epoch [2/5], Step [500/1875], Loss : 1.6721\n",
      "Epoch [2/5], Step [600/1875], Loss : 1.7560\n",
      "Epoch [2/5], Step [700/1875], Loss : 1.7328\n",
      "Epoch [2/5], Step [800/1875], Loss : 1.7046\n",
      "Epoch [2/5], Step [900/1875], Loss : 1.6387\n",
      "Epoch [2/5], Step [1000/1875], Loss : 1.7072\n",
      "Epoch [2/5], Step [1100/1875], Loss : 1.7560\n",
      "Epoch [2/5], Step [1200/1875], Loss : 1.7231\n",
      "Epoch [2/5], Step [1300/1875], Loss : 1.6532\n",
      "Epoch [2/5], Step [1400/1875], Loss : 1.7802\n",
      "Epoch [2/5], Step [1500/1875], Loss : 1.6532\n",
      "Epoch [2/5], Step [1600/1875], Loss : 1.6374\n",
      "Epoch [2/5], Step [1700/1875], Loss : 1.6688\n",
      "Epoch [2/5], Step [1800/1875], Loss : 1.6028\n",
      "Epoch [3/5], Step [100/1875], Loss : 1.6852\n",
      "Epoch [3/5], Step [200/1875], Loss : 1.6234\n",
      "Epoch [3/5], Step [300/1875], Loss : 1.6575\n",
      "Epoch [3/5], Step [400/1875], Loss : 1.6943\n",
      "Epoch [3/5], Step [500/1875], Loss : 1.6799\n",
      "Epoch [3/5], Step [600/1875], Loss : 1.6438\n",
      "Epoch [3/5], Step [700/1875], Loss : 1.6526\n",
      "Epoch [3/5], Step [800/1875], Loss : 1.6145\n",
      "Epoch [3/5], Step [900/1875], Loss : 1.7846\n",
      "Epoch [3/5], Step [1000/1875], Loss : 1.6099\n",
      "Epoch [3/5], Step [1100/1875], Loss : 1.5961\n",
      "Epoch [3/5], Step [1200/1875], Loss : 1.5884\n",
      "Epoch [3/5], Step [1300/1875], Loss : 1.5512\n",
      "Epoch [3/5], Step [1400/1875], Loss : 1.6327\n",
      "Epoch [3/5], Step [1500/1875], Loss : 1.5968\n",
      "Epoch [3/5], Step [1600/1875], Loss : 1.5584\n",
      "Epoch [3/5], Step [1700/1875], Loss : 1.6066\n",
      "Epoch [3/5], Step [1800/1875], Loss : 1.6029\n",
      "Epoch [4/5], Step [100/1875], Loss : 1.5775\n",
      "Epoch [4/5], Step [200/1875], Loss : 1.6941\n",
      "Epoch [4/5], Step [300/1875], Loss : 1.5901\n",
      "Epoch [4/5], Step [400/1875], Loss : 1.5376\n",
      "Epoch [4/5], Step [500/1875], Loss : 1.5909\n",
      "Epoch [4/5], Step [600/1875], Loss : 1.5352\n",
      "Epoch [4/5], Step [700/1875], Loss : 1.5438\n",
      "Epoch [4/5], Step [800/1875], Loss : 1.6059\n",
      "Epoch [4/5], Step [900/1875], Loss : 1.5944\n",
      "Epoch [4/5], Step [1000/1875], Loss : 1.5317\n",
      "Epoch [4/5], Step [1100/1875], Loss : 1.5372\n",
      "Epoch [4/5], Step [1200/1875], Loss : 1.5328\n",
      "Epoch [4/5], Step [1300/1875], Loss : 1.5060\n",
      "Epoch [4/5], Step [1400/1875], Loss : 1.5436\n",
      "Epoch [4/5], Step [1500/1875], Loss : 1.5606\n",
      "Epoch [4/5], Step [1600/1875], Loss : 1.5613\n",
      "Epoch [4/5], Step [1700/1875], Loss : 1.6269\n",
      "Epoch [4/5], Step [1800/1875], Loss : 1.5944\n",
      "Epoch [5/5], Step [100/1875], Loss : 1.5517\n",
      "Epoch [5/5], Step [200/1875], Loss : 1.5106\n",
      "Epoch [5/5], Step [300/1875], Loss : 1.5239\n",
      "Epoch [5/5], Step [400/1875], Loss : 1.5048\n",
      "Epoch [5/5], Step [500/1875], Loss : 1.5305\n",
      "Epoch [5/5], Step [600/1875], Loss : 1.5558\n",
      "Epoch [5/5], Step [700/1875], Loss : 1.5468\n",
      "Epoch [5/5], Step [800/1875], Loss : 1.6413\n",
      "Epoch [5/5], Step [900/1875], Loss : 1.5473\n",
      "Epoch [5/5], Step [1000/1875], Loss : 1.5269\n",
      "Epoch [5/5], Step [1100/1875], Loss : 1.5323\n",
      "Epoch [5/5], Step [1200/1875], Loss : 1.5559\n",
      "Epoch [5/5], Step [1300/1875], Loss : 1.5739\n",
      "Epoch [5/5], Step [1400/1875], Loss : 1.5906\n",
      "Epoch [5/5], Step [1500/1875], Loss : 1.5130\n",
      "Epoch [5/5], Step [1600/1875], Loss : 1.5081\n",
      "Epoch [5/5], Step [1700/1875], Loss : 1.5555\n",
      "Epoch [5/5], Step [1800/1875], Loss : 1.4915\n"
     ]
    }
   ],
   "source": [
    "model = Neuralnetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (img, label) in enumerate(train_loader,1):\n",
    "        img, label = Variable(img), Variable(label)\n",
    "        out = model(img)\n",
    "        loss = criterion(out,label)\n",
    "        \n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx+1) % 100 ==0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], Loss : {:.4f}\".format(epoch+1, num_epochs, idx+1, len(train_loader), loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the Model on the 10000 test images : 95.13 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for img, label in test_loader:\n",
    "    out = model(img)\n",
    "    _, predicted = torch.max(out.data,1)\n",
    "    total += label.size(0)\n",
    "    correct+= (predicted == label).sum().item()\n",
    "\n",
    "\n",
    "print('Test Accuracy of the Model on the 10000 test images : {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제 : CIFAR - 10에 CNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# CIFAR-10 Dataset DataLoader\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                        transform=transforms.ToTensor())\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, \n",
    "                                       transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5), # kernal size = filter size. 16은 필터 개수와 동일. 여기서는 5만 넣어도 5*5로 인식함.\n",
    "            # output channel은 임의로 바꿔 줘도 된다. 다만 그 다음 conv2d에서 input 맞춰줘야 함)\n",
    "            \n",
    "            #nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0), # kernal size = filter size. 16은 필터 개수와 동일.\n",
    "#             nn.BatchNorm2d(6),\n",
    "            # RELU의 기준점은 0. 전체적으로 -값을 batchnorm을 통해 정규분포화하는 것. \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(-1, 16 * 5 * 5) #\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = CNN()\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.252\n",
      "[1,  4000] loss: 2.041\n",
      "[1,  6000] loss: 1.894\n",
      "[1,  8000] loss: 1.739\n",
      "[1, 10000] loss: 1.638\n",
      "[1, 12000] loss: 1.567\n",
      "[2,  2000] loss: 1.503\n",
      "[2,  4000] loss: 1.474\n",
      "[2,  6000] loss: 1.460\n",
      "[2,  8000] loss: 1.423\n",
      "[2, 10000] loss: 1.407\n",
      "[2, 12000] loss: 1.394\n",
      "[3,  2000] loss: 1.351\n",
      "[3,  4000] loss: 1.334\n",
      "[3,  6000] loss: 1.305\n",
      "[3,  8000] loss: 1.313\n",
      "[3, 10000] loss: 1.280\n",
      "[3, 12000] loss: 1.273\n",
      "[4,  2000] loss: 1.224\n",
      "[4,  4000] loss: 1.209\n",
      "[4,  6000] loss: 1.212\n",
      "[4,  8000] loss: 1.220\n",
      "[4, 10000] loss: 1.202\n",
      "[4, 12000] loss: 1.197\n",
      "[5,  2000] loss: 1.154\n",
      "[5,  4000] loss: 1.129\n",
      "[5,  6000] loss: 1.136\n",
      "[5,  8000] loss: 1.137\n",
      "[5, 10000] loss: 1.139\n",
      "[5, 12000] loss: 1.140\n"
     ]
    }
   ],
   "source": [
    "# 오류를 보니 conv2d의 input, output layer 설정에 오류가 있는 것 같은데, 어디를 어떻게 수정해야 할 지 모르겠습니다.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 58.41 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for img, label in test_loader:\n",
    "    out = model(img)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    total += label.size(0)\n",
    "    correct += (predicted == label).sum().item()\n",
    "    \n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model_nn_cifar10.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
